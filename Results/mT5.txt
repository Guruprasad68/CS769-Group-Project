Data Used : BAKSA

T5 Tokenizer Loaded...
Max input length: 150
Data loading complete
Number of training examples: 9800
Number of validation examples: 4200
Number of test examples: 3131
defaultdict(None, {'1': 0, '2': 1, '0': 2})
Device in use: cuda
Iterators created
Downloading MT5 model...
Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5EncoderModel: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.final_layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight']
- This IS expected if you are initializing MT5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MT5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
MT5 model downloaded
The BERTCNNSentiment(
  (bert): MT5EncoderModel(
    (shared): Embedding(250112, 512)
    (encoder): T5Stack(
      (embed_tokens): Embedding(250112, 512)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
                (relative_attention_bias): Embedding(32, 6)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (2): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (6): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (conv_0): Conv2d(1, 100, kernel_size=(2, 512), stride=(1, 1))
  (conv_1): Conv2d(1, 100, kernel_size=(3, 512), stride=(1, 1))
  (conv_2): Conv2d(1, 100, kernel_size=(4, 512), stride=(1, 1))
  (fc): Linear(in_features=300, out_features=3, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
) has 147,402,611 trainable parameters
The AttentionModel(
  (bert): MT5EncoderModel(
    (shared): Embedding(250112, 512)
    (encoder): T5Stack(
      (embed_tokens): Embedding(250112, 512)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
                (relative_attention_bias): Embedding(32, 6)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (2): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (5): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (6): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (7): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=512, out_features=384, bias=False)
                (k): Linear(in_features=512, out_features=384, bias=False)
                (v): Linear(in_features=512, out_features=384, bias=False)
                (o): Linear(in_features=384, out_features=512, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseGatedGeluDense(
                (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                (wo): Linear(in_features=1024, out_features=512, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (gelu_act): NewGELUActivation()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (lstm): LSTM(512, 100)
  (label): Linear(in_features=100, out_features=3, bias=True)
) has 147,186,511 trainable parameters
Parameters for CNN_Model
bert.shared.weight
bert.encoder.block.0.layer.0.SelfAttention.q.weight
bert.encoder.block.0.layer.0.SelfAttention.k.weight
bert.encoder.block.0.layer.0.SelfAttention.v.weight
bert.encoder.block.0.layer.0.SelfAttention.o.weight
bert.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight
bert.encoder.block.0.layer.0.layer_norm.weight
bert.encoder.block.0.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.0.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.0.layer.1.DenseReluDense.wo.weight
bert.encoder.block.0.layer.1.layer_norm.weight
bert.encoder.block.1.layer.0.SelfAttention.q.weight
bert.encoder.block.1.layer.0.SelfAttention.k.weight
bert.encoder.block.1.layer.0.SelfAttention.v.weight
bert.encoder.block.1.layer.0.SelfAttention.o.weight
bert.encoder.block.1.layer.0.layer_norm.weight
bert.encoder.block.1.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.1.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.1.layer.1.DenseReluDense.wo.weight
bert.encoder.block.1.layer.1.layer_norm.weight
bert.encoder.block.2.layer.0.SelfAttention.q.weight
bert.encoder.block.2.layer.0.SelfAttention.k.weight
bert.encoder.block.2.layer.0.SelfAttention.v.weight
bert.encoder.block.2.layer.0.SelfAttention.o.weight
bert.encoder.block.2.layer.0.layer_norm.weight
bert.encoder.block.2.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.2.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.2.layer.1.DenseReluDense.wo.weight
bert.encoder.block.2.layer.1.layer_norm.weight
bert.encoder.block.3.layer.0.SelfAttention.q.weight
bert.encoder.block.3.layer.0.SelfAttention.k.weight
bert.encoder.block.3.layer.0.SelfAttention.v.weight
bert.encoder.block.3.layer.0.SelfAttention.o.weight
bert.encoder.block.3.layer.0.layer_norm.weight
bert.encoder.block.3.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.3.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.3.layer.1.DenseReluDense.wo.weight
bert.encoder.block.3.layer.1.layer_norm.weight
bert.encoder.block.4.layer.0.SelfAttention.q.weight
bert.encoder.block.4.layer.0.SelfAttention.k.weight
bert.encoder.block.4.layer.0.SelfAttention.v.weight
bert.encoder.block.4.layer.0.SelfAttention.o.weight
bert.encoder.block.4.layer.0.layer_norm.weight
bert.encoder.block.4.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.4.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.4.layer.1.DenseReluDense.wo.weight
bert.encoder.block.4.layer.1.layer_norm.weight
bert.encoder.block.5.layer.0.SelfAttention.q.weight
bert.encoder.block.5.layer.0.SelfAttention.k.weight
bert.encoder.block.5.layer.0.SelfAttention.v.weight
bert.encoder.block.5.layer.0.SelfAttention.o.weight
bert.encoder.block.5.layer.0.layer_norm.weight
bert.encoder.block.5.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.5.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.5.layer.1.DenseReluDense.wo.weight
bert.encoder.block.5.layer.1.layer_norm.weight
bert.encoder.block.6.layer.0.SelfAttention.q.weight
bert.encoder.block.6.layer.0.SelfAttention.k.weight
bert.encoder.block.6.layer.0.SelfAttention.v.weight
bert.encoder.block.6.layer.0.SelfAttention.o.weight
bert.encoder.block.6.layer.0.layer_norm.weight
bert.encoder.block.6.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.6.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.6.layer.1.DenseReluDense.wo.weight
bert.encoder.block.6.layer.1.layer_norm.weight
bert.encoder.block.7.layer.0.SelfAttention.q.weight
bert.encoder.block.7.layer.0.SelfAttention.k.weight
bert.encoder.block.7.layer.0.SelfAttention.v.weight
bert.encoder.block.7.layer.0.SelfAttention.o.weight
bert.encoder.block.7.layer.0.layer_norm.weight
bert.encoder.block.7.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.7.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.7.layer.1.DenseReluDense.wo.weight
bert.encoder.block.7.layer.1.layer_norm.weight
bert.encoder.final_layer_norm.weight
conv_0.weight
conv_0.bias
conv_1.weight
conv_1.bias
conv_2.weight
conv_2.bias
fc.weight
fc.bias
Parameters for Attention_Model
bert.shared.weight
bert.encoder.block.0.layer.0.SelfAttention.q.weight
bert.encoder.block.0.layer.0.SelfAttention.k.weight
bert.encoder.block.0.layer.0.SelfAttention.v.weight
bert.encoder.block.0.layer.0.SelfAttention.o.weight
bert.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight
bert.encoder.block.0.layer.0.layer_norm.weight
bert.encoder.block.0.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.0.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.0.layer.1.DenseReluDense.wo.weight
bert.encoder.block.0.layer.1.layer_norm.weight
bert.encoder.block.1.layer.0.SelfAttention.q.weight
bert.encoder.block.1.layer.0.SelfAttention.k.weight
bert.encoder.block.1.layer.0.SelfAttention.v.weight
bert.encoder.block.1.layer.0.SelfAttention.o.weight
bert.encoder.block.1.layer.0.layer_norm.weight
bert.encoder.block.1.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.1.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.1.layer.1.DenseReluDense.wo.weight
bert.encoder.block.1.layer.1.layer_norm.weight
bert.encoder.block.2.layer.0.SelfAttention.q.weight
bert.encoder.block.2.layer.0.SelfAttention.k.weight
bert.encoder.block.2.layer.0.SelfAttention.v.weight
bert.encoder.block.2.layer.0.SelfAttention.o.weight
bert.encoder.block.2.layer.0.layer_norm.weight
bert.encoder.block.2.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.2.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.2.layer.1.DenseReluDense.wo.weight
bert.encoder.block.2.layer.1.layer_norm.weight
bert.encoder.block.3.layer.0.SelfAttention.q.weight
bert.encoder.block.3.layer.0.SelfAttention.k.weight
bert.encoder.block.3.layer.0.SelfAttention.v.weight
bert.encoder.block.3.layer.0.SelfAttention.o.weight
bert.encoder.block.3.layer.0.layer_norm.weight
bert.encoder.block.3.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.3.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.3.layer.1.DenseReluDense.wo.weight
bert.encoder.block.3.layer.1.layer_norm.weight
bert.encoder.block.4.layer.0.SelfAttention.q.weight
bert.encoder.block.4.layer.0.SelfAttention.k.weight
bert.encoder.block.4.layer.0.SelfAttention.v.weight
bert.encoder.block.4.layer.0.SelfAttention.o.weight
bert.encoder.block.4.layer.0.layer_norm.weight
bert.encoder.block.4.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.4.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.4.layer.1.DenseReluDense.wo.weight
bert.encoder.block.4.layer.1.layer_norm.weight
bert.encoder.block.5.layer.0.SelfAttention.q.weight
bert.encoder.block.5.layer.0.SelfAttention.k.weight
bert.encoder.block.5.layer.0.SelfAttention.v.weight
bert.encoder.block.5.layer.0.SelfAttention.o.weight
bert.encoder.block.5.layer.0.layer_norm.weight
bert.encoder.block.5.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.5.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.5.layer.1.DenseReluDense.wo.weight
bert.encoder.block.5.layer.1.layer_norm.weight
bert.encoder.block.6.layer.0.SelfAttention.q.weight
bert.encoder.block.6.layer.0.SelfAttention.k.weight
bert.encoder.block.6.layer.0.SelfAttention.v.weight
bert.encoder.block.6.layer.0.SelfAttention.o.weight
bert.encoder.block.6.layer.0.layer_norm.weight
bert.encoder.block.6.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.6.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.6.layer.1.DenseReluDense.wo.weight
bert.encoder.block.6.layer.1.layer_norm.weight
bert.encoder.block.7.layer.0.SelfAttention.q.weight
bert.encoder.block.7.layer.0.SelfAttention.k.weight
bert.encoder.block.7.layer.0.SelfAttention.v.weight
bert.encoder.block.7.layer.0.SelfAttention.o.weight
bert.encoder.block.7.layer.0.layer_norm.weight
bert.encoder.block.7.layer.1.DenseReluDense.wi_0.weight
bert.encoder.block.7.layer.1.DenseReluDense.wi_1.weight
bert.encoder.block.7.layer.1.DenseReluDense.wo.weight
bert.encoder.block.7.layer.1.layer_norm.weight
bert.encoder.final_layer_norm.weight
lstm.weight_ih_l0
lstm.weight_hh_l0
lstm.bias_ih_l0
lstm.bias_hh_l0
label.weight
label.bias
../checkpoint/cnn_model.txt
Epoch: 01 | Epoch Time: 0m 23s
	Train Loss: 1.057 | Train Acc: 43.40%
	 Val. Loss: 0.986 |  Val. Acc: 49.10%
tensor([0.1924, 0.5343, 0.7877, 0.4561])
tensor([[318., 382., 917.],
        [193., 753., 409.],
        [158.,  76., 994.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 01 | Epoch Time: 0m 22s
	Train Loss: 1.066 | Train Acc: 41.16%
	 Val. Loss: 1.018 |  Val. Acc: 46.62%
tensor([0.4510, 0.6749, 0.2451, 0.4323])
tensor([[714., 668., 235.],
        [336., 939.,  80.],
        [621., 302., 305.]], dtype=torch.float64)
Epoch: 02 | Epoch Time: 0m 23s
	Train Loss: 1.002 | Train Acc: 48.13%
	 Val. Loss: 0.969 |  Val. Acc: 50.32%
tensor([0.1114, 0.6381, 0.8349, 0.4514])
tensor([[ 185.,  455.,  977.],
        [  96.,  887.,  372.],
        [  77.,  107., 1044.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 02 | Epoch Time: 0m 22s
	Train Loss: 0.999 | Train Acc: 48.09%
	 Val. Loss: 1.010 |  Val. Acc: 47.01%
tensor([0.3841, 0.6733, 0.3477, 0.4439])
tensor([[612., 669., 336.],
        [307., 940., 108.],
        [503., 302., 423.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 03 | Epoch Time: 0m 23s
	Train Loss: 0.978 | Train Acc: 50.93%
	 Val. Loss: 0.924 |  Val. Acc: 54.48%
tensor([0.3283, 0.6313, 0.6987, 0.5273])
tensor([[532., 441., 644.],
        [251., 882., 222.],
        [247., 105., 876.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 03 | Epoch Time: 0m 22s
	Train Loss: 0.977 | Train Acc: 50.56%
	 Val. Loss: 0.988 |  Val. Acc: 49.87%
tensor([0.5241, 0.5145, 0.4351, 0.4823])
tensor([[836., 372., 409.],
        [488., 719., 148.],
        [545., 142., 541.]], dtype=torch.float64)
Epoch: 04 | Epoch Time: 0m 23s
	Train Loss: 0.969 | Train Acc: 51.49%
	 Val. Loss: 0.931 |  Val. Acc: 53.44%
tensor([0.4533, 0.5593, 0.6005, 0.5260])
tensor([[725., 367., 525.],
        [410., 775., 170.],
        [399.,  83., 746.]], dtype=torch.float64)
Epoch: 04 | Epoch Time: 0m 22s
	Train Loss: 0.962 | Train Acc: 52.06%
	 Val. Loss: 0.986 |  Val. Acc: 49.08%
tensor([0.6408, 0.5494, 0.2312, 0.4524])
tensor([[1019.,  436.,  162.],
        [ 528.,  767.,   60.],
        [ 786.,  166.,  276.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 05 | Epoch Time: 0m 23s
	Train Loss: 0.953 | Train Acc: 51.99%
	 Val. Loss: 0.920 |  Val. Acc: 54.53%
tensor([0.4634, 0.6856, 0.4801, 0.5353])
tensor([[744., 518., 355.],
        [294., 946., 115.],
        [496., 130., 602.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 05 | Epoch Time: 0m 22s
	Train Loss: 0.963 | Train Acc: 51.43%
	 Val. Loss: 0.982 |  Val. Acc: 51.22%
tensor([0.5175, 0.6493, 0.3450, 0.4950])
tensor([[828., 549., 240.],
        [346., 901., 108.],
        [550., 254., 424.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 06 | Epoch Time: 0m 23s
	Train Loss: 0.943 | Train Acc: 53.59%
	 Val. Loss: 0.911 |  Val. Acc: 55.24%
tensor([0.5175, 0.5741, 0.5582, 0.5472])
tensor([[834., 344., 439.],
        [410., 794., 151.],
        [472.,  62., 694.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 06 | Epoch Time: 0m 23s
	Train Loss: 0.949 | Train Acc: 53.07%
	 Val. Loss: 0.975 |  Val. Acc: 51.93%
tensor([0.5238, 0.5933, 0.4221, 0.5082])
tensor([[841., 463., 313.],
        [379., 826., 150.],
        [553., 159., 516.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 07 | Epoch Time: 0m 23s
	Train Loss: 0.942 | Train Acc: 53.26%
	 Val. Loss: 0.911 |  Val. Acc: 55.59%
tensor([0.4940, 0.6494, 0.5173, 0.5486])
tensor([[794., 431., 392.],
        [332., 895., 128.],
        [484.,  97., 647.]], dtype=torch.float64)
Epoch: 07 | Epoch Time: 0m 22s
	Train Loss: 0.947 | Train Acc: 53.10%
	 Val. Loss: 0.974 |  Val. Acc: 49.84%
tensor([0.7811, 0.4686, 0.1516, 0.4392])
tensor([[1252.,  284.,   81.],
        [ 646.,  660.,   49.],
        [ 954.,   91.,  183.]], dtype=torch.float64)
Epoch: 08 | Epoch Time: 0m 23s
	Train Loss: 0.928 | Train Acc: 54.18%
	 Val. Loss: 0.914 |  Val. Acc: 55.26%
tensor([0.4428, 0.6840, 0.5378, 0.5451])
tensor([[711., 494., 412.],
        [285., 941., 129.],
        [431., 127., 670.]], dtype=torch.float64)
Epoch: 08 | Epoch Time: 0m 22s
	Train Loss: 0.944 | Train Acc: 53.37%
	 Val. Loss: 0.968 |  Val. Acc: 51.62%
tensor([0.6870, 0.4229, 0.3875, 0.4952])
tensor([[1100.,  243.,  274.],
        [ 631.,  589.,  135.],
        [ 677.,   71.,  480.]], dtype=torch.float64)
Epoch: 09 | Epoch Time: 0m 23s
	Train Loss: 0.921 | Train Acc: 54.71%
	 Val. Loss: 0.936 |  Val. Acc: 55.47%
tensor([0.2944, 0.7510, 0.6684, 0.5358])
tensor([[ 472.,  604.,  541.],
        [ 145., 1030.,  180.],
        [ 232.,  167.,  829.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 09 | Epoch Time: 0m 22s
	Train Loss: 0.940 | Train Acc: 53.27%
	 Val. Loss: 0.957 |  Val. Acc: 54.21%
tensor([0.4152, 0.5874, 0.6248, 0.5300])
tensor([[670., 428., 519.],
        [301., 817., 237.],
        [301., 136., 791.]], dtype=torch.float64)
Epoch: 10 | Epoch Time: 0m 23s
	Train Loss: 0.916 | Train Acc: 55.32%
	 Val. Loss: 0.953 |  Val. Acc: 54.37%
tensor([0.3450, 0.7873, 0.5157, 0.5266])
tensor([[ 559.,  696.,  362.],
        [ 168., 1077.,  110.],
        [ 333.,  246.,  649.]], dtype=torch.float64)
Epoch: 10 | Epoch Time: 0m 22s
	Train Loss: 0.937 | Train Acc: 53.87%
	 Val. Loss: 0.957 |  Val. Acc: 53.51%
tensor([0.5601, 0.5282, 0.4892, 0.5233])
tensor([[900., 351., 366.],
        [447., 737., 171.],
        [516., 100., 612.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 11 | Epoch Time: 0m 23s
	Train Loss: 0.920 | Train Acc: 55.08%
	 Val. Loss: 0.932 |  Val. Acc: 56.42%
tensor([0.3294, 0.7392, 0.6687, 0.5495])
tensor([[ 531.,  560.,  526.],
        [ 172., 1015.,  168.],
        [ 250.,  153.,  825.]], dtype=torch.float64)
Epoch: 11 | Epoch Time: 0m 22s
	Train Loss: 0.930 | Train Acc: 54.35%
	 Val. Loss: 0.979 |  Val. Acc: 50.40%
tensor([0.7748, 0.4376, 0.2173, 0.4563])
tensor([[1242.,  255.,  120.],
        [ 678.,  617.,   60.],
        [ 904.,   66.,  258.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 12 | Epoch Time: 0m 23s
	Train Loss: 0.909 | Train Acc: 55.95%
	 Val. Loss: 0.909 |  Val. Acc: 57.04%
tensor([0.5263, 0.5990, 0.5882, 0.5677])
tensor([[850., 344., 423.],
        [395., 825., 135.],
        [450.,  56., 722.]], dtype=torch.float64)
Epoch: 12 | Epoch Time: 0m 22s
	Train Loss: 0.923 | Train Acc: 54.88%
	 Val. Loss: 0.997 |  Val. Acc: 50.64%
tensor([0.6937, 0.3390, 0.4371, 0.4801])
tensor([[1113.,  172.,  332.],
        [ 727.,  478.,  150.],
        [ 656.,   35.,  537.]], dtype=torch.float64)
Epoch: 13 | Epoch Time: 0m 23s
	Train Loss: 0.901 | Train Acc: 56.54%
	 Val. Loss: 0.927 |  Val. Acc: 56.79%
tensor([0.3886, 0.7159, 0.6318, 0.5585])
tensor([[630., 512., 475.],
        [222., 980., 153.],
        [311., 141., 776.]], dtype=torch.float64)
Epoch: 13 | Epoch Time: 0m 22s
	Train Loss: 0.924 | Train Acc: 54.79%
	 Val. Loss: 0.959 |  Val. Acc: 53.54%
tensor([0.6177, 0.4736, 0.4814, 0.5230])
tensor([[993., 264., 360.],
        [541., 658., 156.],
        [558.,  70., 600.]], dtype=torch.float64)
Epoch: 14 | Epoch Time: 0m 23s
	Train Loss: 0.902 | Train Acc: 55.79%
	 Val. Loss: 0.923 |  Val. Acc: 56.34%
tensor([0.4319, 0.7104, 0.5633, 0.5560])
tensor([[698., 517., 402.],
        [264., 972., 119.],
        [386., 145., 697.]], dtype=torch.float64)
Epoch: 14 | Epoch Time: 0m 22s
	Train Loss: 0.923 | Train Acc: 54.34%
	 Val. Loss: 0.954 |  Val. Acc: 52.75%
tensor([0.5958, 0.5557, 0.3914, 0.5137])
tensor([[959., 395., 263.],
        [477., 771., 107.],
        [597., 144., 487.]], dtype=torch.float64)
Epoch: 15 | Epoch Time: 0m 22s
	Train Loss: 0.903 | Train Acc: 57.22%
	 Val. Loss: 0.923 |  Val. Acc: 56.51%
tensor([0.4932, 0.6984, 0.5104, 0.5589])
tensor([[793., 491., 333.],
        [300., 954., 101.],
        [463., 138., 627.]], dtype=torch.float64)
Epoch: 15 | Epoch Time: 0m 22s
	Train Loss: 0.923 | Train Acc: 54.98%
	 Val. Loss: 0.969 |  Val. Acc: 51.83%
tensor([0.6960, 0.5482, 0.2536, 0.4853])
tensor([[1113.,  353.,  151.],
        [ 532.,  759.,   64.],
        [ 817.,  106.,  305.]], dtype=torch.float64)
Epoch: 16 | Epoch Time: 0m 23s
	Train Loss: 0.896 | Train Acc: 56.81%
	 Val. Loss: 0.920 |  Val. Acc: 56.37%
tensor([0.5478, 0.6336, 0.5027, 0.5591])
tensor([[882., 411., 324.],
        [388., 868.,  99.],
        [510.,  99., 619.]], dtype=torch.float64)
Epoch: 16 | Epoch Time: 0m 22s
	Train Loss: 0.914 | Train Acc: 55.88%
	 Val. Loss: 0.945 |  Val. Acc: 53.32%
tensor([0.5508, 0.5909, 0.4213, 0.5210])
tensor([[891., 403., 323.],
        [412., 821., 122.],
        [570., 130., 528.]], dtype=torch.float64)
Epoch: 17 | Epoch Time: 0m 23s
	Train Loss: 0.892 | Train Acc: 57.42%
	 Val. Loss: 0.918 |  Val. Acc: 56.60%
tensor([0.5664, 0.6229, 0.4947, 0.5601])
tensor([[910., 395., 312.],
        [403., 855.,  97.],
        [533.,  83., 612.]], dtype=torch.float64)
Epoch: 17 | Epoch Time: 0m 22s
	Train Loss: 0.912 | Train Acc: 55.97%
	 Val. Loss: 0.979 |  Val. Acc: 52.07%
tensor([0.6918, 0.5469, 0.2687, 0.4924])
tensor([[1110.,  341.,  166.],
        [ 525.,  757.,   73.],
        [ 782.,  125.,  321.]], dtype=torch.float64)
Epoch: 18 | Epoch Time: 0m 22s
	Train Loss: 0.895 | Train Acc: 57.41%
	 Val. Loss: 0.963 |  Val. Acc: 56.27%
tensor([0.3205, 0.6192, 0.7939, 0.5445])
tensor([[527., 380., 710.],
        [266., 848., 241.],
        [172.,  67., 989.]], dtype=torch.float64)
Epoch: 18 | Epoch Time: 0m 22s
	Train Loss: 0.907 | Train Acc: 56.21%
	 Val. Loss: 0.980 |  Val. Acc: 51.68%
tensor([0.6634, 0.6128, 0.2296, 0.4823])
tensor([[1061.,  423.,  133.],
        [ 467.,  842.,   46.],
        [ 821.,  139.,  268.]], dtype=torch.float64)
Epoch: 19 | Epoch Time: 0m 23s
	Train Loss: 0.892 | Train Acc: 56.44%
	 Val. Loss: 0.937 |  Val. Acc: 56.30%
tensor([0.3449, 0.6666, 0.7223, 0.5507])
tensor([[560., 432., 625.],
        [244., 912., 199.],
        [238.,  97., 893.]], dtype=torch.float64)
Epoch: 19 | Epoch Time: 0m 22s
	Train Loss: 0.909 | Train Acc: 55.52%
	 Val. Loss: 0.963 |  Val. Acc: 52.68%
tensor([0.6689, 0.5780, 0.2873, 0.5010])
tensor([[1072.,  375.,  170.],
        [ 489.,  796.,   70.],
        [ 766.,  117.,  345.]], dtype=torch.float64)
Epoch: 20 | Epoch Time: 0m 23s
	Train Loss: 0.880 | Train Acc: 58.25%
	 Val. Loss: 0.943 |  Val. Acc: 56.47%
tensor([0.3559, 0.6456, 0.7306, 0.5528])
tensor([[581., 418., 618.],
        [273., 882., 200.],
        [237.,  82., 909.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 20 | Epoch Time: 0m 21s
	Train Loss: 0.905 | Train Acc: 56.18%
	 Val. Loss: 0.935 |  Val. Acc: 54.53%
tensor([0.5580, 0.5463, 0.5086, 0.5379])
tensor([[902., 330., 385.],
        [446., 754., 155.],
        [505.,  88., 635.]], dtype=torch.float64)
Epoch: 21 | Epoch Time: 0m 23s
	Train Loss: 0.874 | Train Acc: 58.01%
	 Val. Loss: 0.929 |  Val. Acc: 57.14%
tensor([0.3943, 0.6843, 0.6616, 0.5618])
tensor([[641., 452., 524.],
        [259., 935., 161.],
        [303., 100., 825.]], dtype=torch.float64)
Epoch: 21 | Epoch Time: 0m 22s
	Train Loss: 0.904 | Train Acc: 56.37%
	 Val. Loss: 1.011 |  Val. Acc: 51.08%
tensor([0.4892, 0.7277, 0.2972, 0.4866])
tensor([[ 787.,  669.,  161.],
        [ 289., 1002.,   64.],
        [ 538.,  332.,  358.]], dtype=torch.float64)
Epoch: 22 | Epoch Time: 0m 23s
	Train Loss: 0.880 | Train Acc: 57.61%
	 Val. Loss: 0.936 |  Val. Acc: 56.89%
tensor([0.4169, 0.6846, 0.6295, 0.5620])
tensor([[675., 477., 465.],
        [281., 934., 140.],
        [334., 113., 781.]], dtype=torch.float64)
Epoch: 22 | Epoch Time: 0m 22s
	Train Loss: 0.902 | Train Acc: 56.82%
	 Val. Loss: 0.966 |  Val. Acc: 53.56%
tensor([0.6615, 0.5354, 0.3700, 0.5193])
tensor([[1062.,  316.,  239.],
        [ 514.,  741.,  100.],
        [ 686.,   96.,  446.]], dtype=torch.float64)
Epoch: 23 | Epoch Time: 0m 23s
	Train Loss: 0.877 | Train Acc: 58.28%
	 Val. Loss: 0.936 |  Val. Acc: 56.99%
tensor([0.4883, 0.6146, 0.6124, 0.5649])
tensor([[792., 388., 437.],
        [373., 842., 140.],
        [388.,  80., 760.]], dtype=torch.float64)
Epoch: 23 | Epoch Time: 0m 22s
	Train Loss: 0.903 | Train Acc: 56.07%
	 Val. Loss: 1.030 |  Val. Acc: 50.56%
tensor([0.5864, 0.6664, 0.2270, 0.4711])
tensor([[945., 573.,  99.],
        [398., 919.,  38.],
        [685., 283., 260.]], dtype=torch.float64)
Epoch: 24 | Epoch Time: 0m 23s
	Train Loss: 0.874 | Train Acc: 58.15%
	 Val. Loss: 0.981 |  Val. Acc: 55.91%
tensor([0.2714, 0.7035, 0.7588, 0.5359])
tensor([[441., 528., 648.],
        [182., 965., 208.],
        [155., 130., 943.]], dtype=torch.float64)
Epoch: 24 | Epoch Time: 0m 22s
	Train Loss: 0.896 | Train Acc: 56.67%
	 Val. Loss: 0.956 |  Val. Acc: 54.29%
tensor([0.4201, 0.6837, 0.5244, 0.5307])
tensor([[680., 545., 392.],
        [261., 946., 148.],
        [378., 194., 656.]], dtype=torch.float64)
Epoch: 25 | Epoch Time: 0m 23s
	Train Loss: 0.869 | Train Acc: 59.27%
	 Val. Loss: 0.959 |  Val. Acc: 56.66%
tensor([0.4433, 0.5533, 0.7259, 0.5590])
tensor([[720., 302., 595.],
        [399., 760., 196.],
        [280.,  48., 900.]], dtype=torch.float64)
Epoch: 25 | Epoch Time: 0m 21s
	Train Loss: 0.891 | Train Acc: 57.27%
	 Val. Loss: 0.956 |  Val. Acc: 52.90%
tensor([0.5972, 0.5823, 0.3783, 0.5155])
tensor([[958., 416., 243.],
        [446., 806., 103.],
        [640., 129., 459.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 26 | Epoch Time: 0m 23s
	Train Loss: 0.864 | Train Acc: 59.42%
	 Val. Loss: 0.937 |  Val. Acc: 57.76%
tensor([0.5216, 0.6536, 0.5609, 0.5720])
tensor([[842., 407., 368.],
        [350., 894., 111.],
        [449.,  89., 690.]], dtype=torch.float64)
Epoch: 26 | Epoch Time: 0m 22s
	Train Loss: 0.890 | Train Acc: 56.79%
	 Val. Loss: 0.951 |  Val. Acc: 54.03%
tensor([0.4985, 0.5791, 0.5371, 0.5331])
tensor([[804., 385., 428.],
        [378., 799., 178.],
        [459., 101., 668.]], dtype=torch.float64)
Epoch: 27 | Epoch Time: 0m 23s
	Train Loss: 0.864 | Train Acc: 59.01%
	 Val. Loss: 0.961 |  Val. Acc: 57.06%
tensor([0.3631, 0.6530, 0.7407, 0.5590])
tensor([[590., 401., 626.],
        [258., 893., 204.],
        [226.,  87., 915.]], dtype=torch.float64)
Epoch: 27 | Epoch Time: 0m 21s
	Train Loss: 0.883 | Train Acc: 57.49%
	 Val. Loss: 0.959 |  Val. Acc: 53.17%
tensor([0.5633, 0.5457, 0.4732, 0.5233])
tensor([[902., 376., 339.],
        [452., 753., 150.],
        [550.,  98., 580.]], dtype=torch.float64)
Epoch: 28 | Epoch Time: 0m 23s
	Train Loss: 0.863 | Train Acc: 59.43%
	 Val. Loss: 0.950 |  Val. Acc: 57.07%
tensor([0.4290, 0.6280, 0.6776, 0.5637])
tensor([[696., 384., 537.],
        [330., 860., 165.],
        [310.,  78., 840.]], dtype=torch.float64)
Epoch: 28 | Epoch Time: 0m 21s
	Train Loss: 0.873 | Train Acc: 58.36%
	 Val. Loss: 0.943 |  Val. Acc: 54.46%
tensor([0.5405, 0.5594, 0.5193, 0.5362])
tensor([[870., 362., 385.],
        [419., 772., 164.],
        [486.,  95., 647.]], dtype=torch.float64)
Epoch: 29 | Epoch Time: 0m 23s
	Train Loss: 0.856 | Train Acc: 59.70%
	 Val. Loss: 0.990 |  Val. Acc: 56.24%
tensor([0.3620, 0.6151, 0.7486, 0.5506])
tensor([[592., 384., 641.],
        [294., 842., 219.],
        [225.,  74., 929.]], dtype=torch.float64)
Epoch: 29 | Epoch Time: 0m 21s
	Train Loss: 0.879 | Train Acc: 58.12%
	 Val. Loss: 0.971 |  Val. Acc: 52.66%
tensor([0.7164, 0.4786, 0.3338, 0.5048])
tensor([[1153.,  270.,  194.],
        [ 618.,  658.,   79.],
        [ 756.,   71.,  401.]], dtype=torch.float64)
Epoch: 30 | Epoch Time: 0m 22s
	Train Loss: 0.857 | Train Acc: 60.09%
	 Val. Loss: 0.953 |  Val. Acc: 57.52%
tensor([0.5435, 0.6671, 0.5021, 0.5693])
tensor([[880., 463., 274.],
        [356., 913.,  86.],
        [486., 119., 623.]], dtype=torch.float64)
../checkpoint/attention_model.txt
Epoch: 30 | Epoch Time: 0m 21s
	Train Loss: 0.881 | Train Acc: 57.86%
	 Val. Loss: 0.950 |  Val. Acc: 55.26%
tensor([0.5559, 0.6034, 0.4796, 0.5445])
tensor([[895., 422., 300.],
        [410., 833., 112.],
        [512., 121., 595.]], dtype=torch.float64)
Epoch: 31 | Epoch Time: 0m 25s
	Train Loss: 0.849 | Train Acc: 59.82%
	 Val. Loss: 0.964 |  Val. Acc: 57.48%
tensor([0.5367, 0.6199, 0.5651, 0.5709])
tensor([[866., 379., 372.],
        [389., 848., 118.],
        [456.,  72., 700.]], dtype=torch.float64)
Epoch: 31 | Epoch Time: 0m 21s
	Train Loss: 0.879 | Train Acc: 58.17%
	 Val. Loss: 0.967 |  Val. Acc: 52.43%
tensor([0.6816, 0.5163, 0.3310, 0.5044])
tensor([[1093.,  335.,  189.],
        [ 545.,  713.,   97.],
        [ 746.,   85.,  397.]], dtype=torch.float64)
Epoch: 32 | Epoch Time: 0m 23s
	Train Loss: 0.845 | Train Acc: 60.60%
	 Val. Loss: 0.972 |  Val. Acc: 57.67%
tensor([0.4370, 0.7179, 0.5875, 0.5677])
tensor([[708., 523., 386.],
        [258., 983., 114.],
        [356., 141., 731.]], dtype=torch.float64)
Epoch: 32 | Epoch Time: 0m 22s
	Train Loss: 0.873 | Train Acc: 58.50%
	 Val. Loss: 0.975 |  Val. Acc: 52.88%
tensor([0.5204, 0.4532, 0.6057, 0.5193])
tensor([[842., 256., 519.],
        [477., 627., 251.],
        [424.,  52., 752.]], dtype=torch.float64)
Epoch: 33 | Epoch Time: 0m 23s
	Train Loss: 0.849 | Train Acc: 60.31%
	 Val. Loss: 0.962 |  Val. Acc: 57.35%
tensor([0.4998, 0.6079, 0.6178, 0.5688])
tensor([[810., 391., 416.],
        [392., 829., 134.],
        [375.,  83., 770.]], dtype=torch.float64)
Epoch: 33 | Epoch Time: 0m 22s
	Train Loss: 0.871 | Train Acc: 58.85%
	 Val. Loss: 0.963 |  Val. Acc: 54.00%
tensor([0.5479, 0.5542, 0.5025, 0.5325])
tensor([[883., 368., 366.],
        [446., 763., 146.],
        [509.,  96., 623.]], dtype=torch.float64)
Epoch: 34 | Epoch Time: 0m 22s
	Train Loss: 0.848 | Train Acc: 60.35%
	 Val. Loss: 0.996 |  Val. Acc: 56.91%
tensor([0.7048, 0.4868, 0.4706, 0.5572])
tensor([[1136.,  235.,  246.],
        [ 606.,  667.,   82.],
        [ 602.,   39.,  587.]], dtype=torch.float64)
Epoch: 34 | Epoch Time: 0m 22s
	Train Loss: 0.872 | Train Acc: 58.22%
	 Val. Loss: 0.995 |  Val. Acc: 54.31%
tensor([0.5173, 0.6574, 0.4365, 0.5322])
tensor([[835., 530., 252.],
        [346., 908., 101.],
        [487., 202., 539.]], dtype=torch.float64)
Epoch: 35 | Epoch Time: 0m 23s
	Train Loss: 0.849 | Train Acc: 59.87%
	 Val. Loss: 0.968 |  Val. Acc: 57.23%
tensor([0.4402, 0.6655, 0.6230, 0.5647])
tensor([[716., 460., 441.],
        [307., 911., 137.],
        [349., 102., 777.]], dtype=torch.float64)
Epoch: 35 | Epoch Time: 0m 21s
	Train Loss: 0.860 | Train Acc: 58.89%
	 Val. Loss: 0.976 |  Val. Acc: 54.26%
tensor([0.5631, 0.5568, 0.4858, 0.5343])
tensor([[909., 378., 330.],
        [458., 767., 130.],
        [515., 109., 604.]], dtype=torch.float64)
../checkpoint/cnn_model.txt
Epoch: 36 | Epoch Time: 0m 23s
	Train Loss: 0.843 | Train Acc: 60.66%
	 Val. Loss: 0.976 |  Val. Acc: 57.85%
tensor([0.5268, 0.5573, 0.6467, 0.5725])
tensor([[857., 313., 447.],
        [446., 767., 142.],
        [369.,  53., 806.]], dtype=torch.float64)
Epoch: 36 | Epoch Time: 0m 22s
	Train Loss: 0.865 | Train Acc: 59.03%
	 Val. Loss: 0.963 |  Val. Acc: 54.76%
tensor([0.6141, 0.5288, 0.4717, 0.5383])
tensor([[993., 318., 306.],
        [497., 728., 130.],
        [569.,  79., 580.]], dtype=torch.float64)
Epoch: 37 | Epoch Time: 0m 22s
	Train Loss: 0.847 | Train Acc: 60.18%
	 Val. Loss: 1.041 |  Val. Acc: 55.65%
tensor([0.2589, 0.7540, 0.7115, 0.5316])
tensor([[ 426.,  622.,  569.],
        [ 155., 1032.,  168.],
        [ 169.,  178.,  881.]], dtype=torch.float64)
Epoch: 37 | Epoch Time: 0m 23s
	Train Loss: 0.861 | Train Acc: 58.72%
	 Val. Loss: 0.982 |  Val. Acc: 54.50%
tensor([0.6444, 0.4727, 0.4818, 0.5337])
tensor([[1042.,  273.,  302.],
        [ 575.,  651.,  129.],
        [ 559.,   73.,  596.]], dtype=torch.float64)
Epoch: 38 | Epoch Time: 0m 24s
	Train Loss: 0.835 | Train Acc: 61.16%
	 Val. Loss: 1.024 |  Val. Acc: 56.83%
tensor([0.3433, 0.6553, 0.7462, 0.5538])
tensor([[559., 434., 624.],
        [249., 901., 205.],
        [213.,  87., 928.]], dtype=torch.float64)
Epoch: 38 | Epoch Time: 0m 22s
	Train Loss: 0.853 | Train Acc: 59.95%
	 Val. Loss: 1.000 |  Val. Acc: 53.42%
tensor([0.6459, 0.4460, 0.4715, 0.5211])
tensor([[1043.,  251.,  323.],
        [ 585.,  618.,  152.],
        [ 589.,   56.,  583.]], dtype=torch.float64)
Epoch: 39 | Epoch Time: 0m 22s
	Train Loss: 0.839 | Train Acc: 61.07%
	 Val. Loss: 0.987 |  Val. Acc: 57.19%
tensor([0.4111, 0.6751, 0.6584, 0.5644])
tensor([[664., 483., 470.],
        [292., 926., 137.],
        [304., 111., 813.]], dtype=torch.float64)
Epoch: 39 | Epoch Time: 0m 22s
	Train Loss: 0.853 | Train Acc: 59.50%
	 Val. Loss: 1.003 |  Val. Acc: 53.76%
tensor([0.5984, 0.4828, 0.5104, 0.5301])
tensor([[965., 284., 368.],
        [527., 665., 163.],
        [532.,  68., 628.]], dtype=torch.float64)
Epoch: 40 | Epoch Time: 0m 22s
	Train Loss: 0.834 | Train Acc: 61.32%
	 Val. Loss: 1.057 |  Val. Acc: 55.96%
tensor([0.2821, 0.6593, 0.7868, 0.5369])
tensor([[464., 457., 696.],
        [217., 905., 233.],
        [168.,  79., 981.]], dtype=torch.float64)
Epoch: 40 | Epoch Time: 0m 22s
	Train Loss: 0.853 | Train Acc: 59.56%
	 Val. Loss: 1.037 |  Val. Acc: 54.18%
tensor([0.3524, 0.6562, 0.6456, 0.5294])
tensor([[570., 517., 530.],
        [249., 900., 206.],
        [252., 170., 806.]], dtype=torch.float64)
tensor([[697., 245., 269.],
        [259., 615.,  67.],
        [343.,  59., 577.]], dtype=torch.float64)
	 Val. Loss: 0.979 |  Val. Acc: 60.35%
tensor([0.5805, 0.6413, 0.5675, 0.5966])