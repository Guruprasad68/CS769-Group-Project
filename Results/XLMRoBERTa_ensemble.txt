xlm-roberta-base Tokenizer Loaded...
Max input length: 150
Data loading complete
Number of training examples: 14000
Number of validation examples: 3000
Number of test examples: 2999
defaultdict(None, {'1': 0, '2': 1, '0': 2})
Device in use: cuda
Iterators created
Downloading xlm-roberta-base model...
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
xlm-roberta-base model downloaded
The BERTCNNSentiment(
  (bert): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (conv_0): Conv2d(1, 100, kernel_size=(2, 768), stride=(1, 1))
  (conv_1): Conv2d(1, 100, kernel_size=(3, 768), stride=(1, 1))
  (conv_2): Conv2d(1, 100, kernel_size=(4, 768), stride=(1, 1))
  (fc): Linear(in_features=300, out_features=3, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
) has 278,736,051 trainable parameters
The AttentionModel(
  (bert): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (lstm): LSTM(768, 100)
  (label): Linear(in_features=100, out_features=3, bias=True)
) has 278,391,951 trainable parameters
Parameters for CNN_Model
bert.embeddings.word_embeddings.weight
bert.embeddings.position_embeddings.weight
bert.embeddings.token_type_embeddings.weight
bert.embeddings.LayerNorm.weight
bert.embeddings.LayerNorm.bias
bert.encoder.layer.0.attention.self.query.weight
bert.encoder.layer.0.attention.self.query.bias
bert.encoder.layer.0.attention.self.key.weight
bert.encoder.layer.0.attention.self.key.bias
bert.encoder.layer.0.attention.self.value.weight
bert.encoder.layer.0.attention.self.value.bias
bert.encoder.layer.0.attention.output.dense.weight
bert.encoder.layer.0.attention.output.dense.bias
bert.encoder.layer.0.attention.output.LayerNorm.weight
bert.encoder.layer.0.attention.output.LayerNorm.bias
bert.encoder.layer.0.intermediate.dense.weight
bert.encoder.layer.0.intermediate.dense.bias
bert.encoder.layer.0.output.dense.weight
bert.encoder.layer.0.output.dense.bias
bert.encoder.layer.0.output.LayerNorm.weight
bert.encoder.layer.0.output.LayerNorm.bias
bert.encoder.layer.1.attention.self.query.weight
bert.encoder.layer.1.attention.self.query.bias
bert.encoder.layer.1.attention.self.key.weight
bert.encoder.layer.1.attention.self.key.bias
bert.encoder.layer.1.attention.self.value.weight
bert.encoder.layer.1.attention.self.value.bias
bert.encoder.layer.1.attention.output.dense.weight
bert.encoder.layer.1.attention.output.dense.bias
bert.encoder.layer.1.attention.output.LayerNorm.weight
bert.encoder.layer.1.attention.output.LayerNorm.bias
bert.encoder.layer.1.intermediate.dense.weight
bert.encoder.layer.1.intermediate.dense.bias
bert.encoder.layer.1.output.dense.weight
bert.encoder.layer.1.output.dense.bias
bert.encoder.layer.1.output.LayerNorm.weight
bert.encoder.layer.1.output.LayerNorm.bias
bert.encoder.layer.2.attention.self.query.weight
bert.encoder.layer.2.attention.self.query.bias
bert.encoder.layer.2.attention.self.key.weight
bert.encoder.layer.2.attention.self.key.bias
bert.encoder.layer.2.attention.self.value.weight
bert.encoder.layer.2.attention.self.value.bias
bert.encoder.layer.2.attention.output.dense.weight
bert.encoder.layer.2.attention.output.dense.bias
bert.encoder.layer.2.attention.output.LayerNorm.weight
bert.encoder.layer.2.attention.output.LayerNorm.bias
bert.encoder.layer.2.intermediate.dense.weight
bert.encoder.layer.2.intermediate.dense.bias
bert.encoder.layer.2.output.dense.weight
bert.encoder.layer.2.output.dense.bias
bert.encoder.layer.2.output.LayerNorm.weight
bert.encoder.layer.2.output.LayerNorm.bias
bert.encoder.layer.3.attention.self.query.weight
bert.encoder.layer.3.attention.self.query.bias
bert.encoder.layer.3.attention.self.key.weight
bert.encoder.layer.3.attention.self.key.bias
bert.encoder.layer.3.attention.self.value.weight
bert.encoder.layer.3.attention.self.value.bias
bert.encoder.layer.3.attention.output.dense.weight
bert.encoder.layer.3.attention.output.dense.bias
bert.encoder.layer.3.attention.output.LayerNorm.weight
bert.encoder.layer.3.attention.output.LayerNorm.bias
bert.encoder.layer.3.intermediate.dense.weight
bert.encoder.layer.3.intermediate.dense.bias
bert.encoder.layer.3.output.dense.weight
bert.encoder.layer.3.output.dense.bias
bert.encoder.layer.3.output.LayerNorm.weight
bert.encoder.layer.3.output.LayerNorm.bias
bert.encoder.layer.4.attention.self.query.weight
bert.encoder.layer.4.attention.self.query.bias
bert.encoder.layer.4.attention.self.key.weight
bert.encoder.layer.4.attention.self.key.bias
bert.encoder.layer.4.attention.self.value.weight
bert.encoder.layer.4.attention.self.value.bias
bert.encoder.layer.4.attention.output.dense.weight
bert.encoder.layer.4.attention.output.dense.bias
bert.encoder.layer.4.attention.output.LayerNorm.weight
bert.encoder.layer.4.attention.output.LayerNorm.bias
bert.encoder.layer.4.intermediate.dense.weight
bert.encoder.layer.4.intermediate.dense.bias
bert.encoder.layer.4.output.dense.weight
bert.encoder.layer.4.output.dense.bias
bert.encoder.layer.4.output.LayerNorm.weight
bert.encoder.layer.4.output.LayerNorm.bias
bert.encoder.layer.5.attention.self.query.weight
bert.encoder.layer.5.attention.self.query.bias
bert.encoder.layer.5.attention.self.key.weight
bert.encoder.layer.5.attention.self.key.bias
bert.encoder.layer.5.attention.self.value.weight
bert.encoder.layer.5.attention.self.value.bias
bert.encoder.layer.5.attention.output.dense.weight
bert.encoder.layer.5.attention.output.dense.bias
bert.encoder.layer.5.attention.output.LayerNorm.weight
bert.encoder.layer.5.attention.output.LayerNorm.bias
bert.encoder.layer.5.intermediate.dense.weight
bert.encoder.layer.5.intermediate.dense.bias
bert.encoder.layer.5.output.dense.weight
bert.encoder.layer.5.output.dense.bias
bert.encoder.layer.5.output.LayerNorm.weight
bert.encoder.layer.5.output.LayerNorm.bias
bert.encoder.layer.6.attention.self.query.weight
bert.encoder.layer.6.attention.self.query.bias
bert.encoder.layer.6.attention.self.key.weight
bert.encoder.layer.6.attention.self.key.bias
bert.encoder.layer.6.attention.self.value.weight
bert.encoder.layer.6.attention.self.value.bias
bert.encoder.layer.6.attention.output.dense.weight
bert.encoder.layer.6.attention.output.dense.bias
bert.encoder.layer.6.attention.output.LayerNorm.weight
bert.encoder.layer.6.attention.output.LayerNorm.bias
bert.encoder.layer.6.intermediate.dense.weight
bert.encoder.layer.6.intermediate.dense.bias
bert.encoder.layer.6.output.dense.weight
bert.encoder.layer.6.output.dense.bias
bert.encoder.layer.6.output.LayerNorm.weight
bert.encoder.layer.6.output.LayerNorm.bias
bert.encoder.layer.7.attention.self.query.weight
bert.encoder.layer.7.attention.self.query.bias
bert.encoder.layer.7.attention.self.key.weight
bert.encoder.layer.7.attention.self.key.bias
bert.encoder.layer.7.attention.self.value.weight
bert.encoder.layer.7.attention.self.value.bias
bert.encoder.layer.7.attention.output.dense.weight
bert.encoder.layer.7.attention.output.dense.bias
bert.encoder.layer.7.attention.output.LayerNorm.weight
bert.encoder.layer.7.attention.output.LayerNorm.bias
bert.encoder.layer.7.intermediate.dense.weight
bert.encoder.layer.7.intermediate.dense.bias
bert.encoder.layer.7.output.dense.weight
bert.encoder.layer.7.output.dense.bias
bert.encoder.layer.7.output.LayerNorm.weight
bert.encoder.layer.7.output.LayerNorm.bias
bert.encoder.layer.8.attention.self.query.weight
bert.encoder.layer.8.attention.self.query.bias
bert.encoder.layer.8.attention.self.key.weight
bert.encoder.layer.8.attention.self.key.bias
bert.encoder.layer.8.attention.self.value.weight
bert.encoder.layer.8.attention.self.value.bias
bert.encoder.layer.8.attention.output.dense.weight
bert.encoder.layer.8.attention.output.dense.bias
bert.encoder.layer.8.attention.output.LayerNorm.weight
bert.encoder.layer.8.attention.output.LayerNorm.bias
bert.encoder.layer.8.intermediate.dense.weight
bert.encoder.layer.8.intermediate.dense.bias
bert.encoder.layer.8.output.dense.weight
bert.encoder.layer.8.output.dense.bias
bert.encoder.layer.8.output.LayerNorm.weight
bert.encoder.layer.8.output.LayerNorm.bias
bert.encoder.layer.9.attention.self.query.weight
bert.encoder.layer.9.attention.self.query.bias
bert.encoder.layer.9.attention.self.key.weight
bert.encoder.layer.9.attention.self.key.bias
bert.encoder.layer.9.attention.self.value.weight
bert.encoder.layer.9.attention.self.value.bias
bert.encoder.layer.9.attention.output.dense.weight
bert.encoder.layer.9.attention.output.dense.bias
bert.encoder.layer.9.attention.output.LayerNorm.weight
bert.encoder.layer.9.attention.output.LayerNorm.bias
bert.encoder.layer.9.intermediate.dense.weight
bert.encoder.layer.9.intermediate.dense.bias
bert.encoder.layer.9.output.dense.weight
bert.encoder.layer.9.output.dense.bias
bert.encoder.layer.9.output.LayerNorm.weight
bert.encoder.layer.9.output.LayerNorm.bias
bert.encoder.layer.10.attention.self.query.weight
bert.encoder.layer.10.attention.self.query.bias
bert.encoder.layer.10.attention.self.key.weight
bert.encoder.layer.10.attention.self.key.bias
bert.encoder.layer.10.attention.self.value.weight
bert.encoder.layer.10.attention.self.value.bias
bert.encoder.layer.10.attention.output.dense.weight
bert.encoder.layer.10.attention.output.dense.bias
bert.encoder.layer.10.attention.output.LayerNorm.weight
bert.encoder.layer.10.attention.output.LayerNorm.bias
bert.encoder.layer.10.intermediate.dense.weight
bert.encoder.layer.10.intermediate.dense.bias
bert.encoder.layer.10.output.dense.weight
bert.encoder.layer.10.output.dense.bias
bert.encoder.layer.10.output.LayerNorm.weight
bert.encoder.layer.10.output.LayerNorm.bias
bert.encoder.layer.11.attention.self.query.weight
bert.encoder.layer.11.attention.self.query.bias
bert.encoder.layer.11.attention.self.key.weight
bert.encoder.layer.11.attention.self.key.bias
bert.encoder.layer.11.attention.self.value.weight
bert.encoder.layer.11.attention.self.value.bias
bert.encoder.layer.11.attention.output.dense.weight
bert.encoder.layer.11.attention.output.dense.bias
bert.encoder.layer.11.attention.output.LayerNorm.weight
bert.encoder.layer.11.attention.output.LayerNorm.bias
bert.encoder.layer.11.intermediate.dense.weight
bert.encoder.layer.11.intermediate.dense.bias
bert.encoder.layer.11.output.dense.weight
bert.encoder.layer.11.output.dense.bias
bert.encoder.layer.11.output.LayerNorm.weight
bert.encoder.layer.11.output.LayerNorm.bias
bert.pooler.dense.weight
bert.pooler.dense.bias
conv_0.weight
conv_0.bias
conv_1.weight
conv_1.bias
conv_2.weight
conv_2.bias
fc.weight
fc.bias
Parameters for Attention_Model
bert.embeddings.word_embeddings.weight
bert.embeddings.position_embeddings.weight
bert.embeddings.token_type_embeddings.weight
bert.embeddings.LayerNorm.weight
bert.embeddings.LayerNorm.bias
bert.encoder.layer.0.attention.self.query.weight
bert.encoder.layer.0.attention.self.query.bias
bert.encoder.layer.0.attention.self.key.weight
bert.encoder.layer.0.attention.self.key.bias
bert.encoder.layer.0.attention.self.value.weight
bert.encoder.layer.0.attention.self.value.bias
bert.encoder.layer.0.attention.output.dense.weight
bert.encoder.layer.0.attention.output.dense.bias
bert.encoder.layer.0.attention.output.LayerNorm.weight
bert.encoder.layer.0.attention.output.LayerNorm.bias
bert.encoder.layer.0.intermediate.dense.weight
bert.encoder.layer.0.intermediate.dense.bias
bert.encoder.layer.0.output.dense.weight
bert.encoder.layer.0.output.dense.bias
bert.encoder.layer.0.output.LayerNorm.weight
bert.encoder.layer.0.output.LayerNorm.bias
bert.encoder.layer.1.attention.self.query.weight
bert.encoder.layer.1.attention.self.query.bias
bert.encoder.layer.1.attention.self.key.weight
bert.encoder.layer.1.attention.self.key.bias
bert.encoder.layer.1.attention.self.value.weight
bert.encoder.layer.1.attention.self.value.bias
bert.encoder.layer.1.attention.output.dense.weight
bert.encoder.layer.1.attention.output.dense.bias
bert.encoder.layer.1.attention.output.LayerNorm.weight
bert.encoder.layer.1.attention.output.LayerNorm.bias
bert.encoder.layer.1.intermediate.dense.weight
bert.encoder.layer.1.intermediate.dense.bias
bert.encoder.layer.1.output.dense.weight
bert.encoder.layer.1.output.dense.bias
bert.encoder.layer.1.output.LayerNorm.weight
bert.encoder.layer.1.output.LayerNorm.bias
bert.encoder.layer.2.attention.self.query.weight
bert.encoder.layer.2.attention.self.query.bias
bert.encoder.layer.2.attention.self.key.weight
bert.encoder.layer.2.attention.self.key.bias
bert.encoder.layer.2.attention.self.value.weight
bert.encoder.layer.2.attention.self.value.bias
bert.encoder.layer.2.attention.output.dense.weight
bert.encoder.layer.2.attention.output.dense.bias
bert.encoder.layer.2.attention.output.LayerNorm.weight
bert.encoder.layer.2.attention.output.LayerNorm.bias
bert.encoder.layer.2.intermediate.dense.weight
bert.encoder.layer.2.intermediate.dense.bias
bert.encoder.layer.2.output.dense.weight
bert.encoder.layer.2.output.dense.bias
bert.encoder.layer.2.output.LayerNorm.weight
bert.encoder.layer.2.output.LayerNorm.bias
bert.encoder.layer.3.attention.self.query.weight
bert.encoder.layer.3.attention.self.query.bias
bert.encoder.layer.3.attention.self.key.weight
bert.encoder.layer.3.attention.self.key.bias
bert.encoder.layer.3.attention.self.value.weight
bert.encoder.layer.3.attention.self.value.bias
bert.encoder.layer.3.attention.output.dense.weight
bert.encoder.layer.3.attention.output.dense.bias
bert.encoder.layer.3.attention.output.LayerNorm.weight
bert.encoder.layer.3.attention.output.LayerNorm.bias
bert.encoder.layer.3.intermediate.dense.weight
bert.encoder.layer.3.intermediate.dense.bias
bert.encoder.layer.3.output.dense.weight
bert.encoder.layer.3.output.dense.bias
bert.encoder.layer.3.output.LayerNorm.weight
bert.encoder.layer.3.output.LayerNorm.bias
bert.encoder.layer.4.attention.self.query.weight
bert.encoder.layer.4.attention.self.query.bias
bert.encoder.layer.4.attention.self.key.weight
bert.encoder.layer.4.attention.self.key.bias
bert.encoder.layer.4.attention.self.value.weight
bert.encoder.layer.4.attention.self.value.bias
bert.encoder.layer.4.attention.output.dense.weight
bert.encoder.layer.4.attention.output.dense.bias
bert.encoder.layer.4.attention.output.LayerNorm.weight
bert.encoder.layer.4.attention.output.LayerNorm.bias
bert.encoder.layer.4.intermediate.dense.weight
bert.encoder.layer.4.intermediate.dense.bias
bert.encoder.layer.4.output.dense.weight
bert.encoder.layer.4.output.dense.bias
bert.encoder.layer.4.output.LayerNorm.weight
bert.encoder.layer.4.output.LayerNorm.bias
bert.encoder.layer.5.attention.self.query.weight
bert.encoder.layer.5.attention.self.query.bias
bert.encoder.layer.5.attention.self.key.weight
bert.encoder.layer.5.attention.self.key.bias
bert.encoder.layer.5.attention.self.value.weight
bert.encoder.layer.5.attention.self.value.bias
bert.encoder.layer.5.attention.output.dense.weight
bert.encoder.layer.5.attention.output.dense.bias
bert.encoder.layer.5.attention.output.LayerNorm.weight
bert.encoder.layer.5.attention.output.LayerNorm.bias
bert.encoder.layer.5.intermediate.dense.weight
bert.encoder.layer.5.intermediate.dense.bias
bert.encoder.layer.5.output.dense.weight
bert.encoder.layer.5.output.dense.bias
bert.encoder.layer.5.output.LayerNorm.weight
bert.encoder.layer.5.output.LayerNorm.bias
bert.encoder.layer.6.attention.self.query.weight
bert.encoder.layer.6.attention.self.query.bias
bert.encoder.layer.6.attention.self.key.weight
bert.encoder.layer.6.attention.self.key.bias
bert.encoder.layer.6.attention.self.value.weight
bert.encoder.layer.6.attention.self.value.bias
bert.encoder.layer.6.attention.output.dense.weight
bert.encoder.layer.6.attention.output.dense.bias
bert.encoder.layer.6.attention.output.LayerNorm.weight
bert.encoder.layer.6.attention.output.LayerNorm.bias
bert.encoder.layer.6.intermediate.dense.weight
bert.encoder.layer.6.intermediate.dense.bias
bert.encoder.layer.6.output.dense.weight
bert.encoder.layer.6.output.dense.bias
bert.encoder.layer.6.output.LayerNorm.weight
bert.encoder.layer.6.output.LayerNorm.bias
bert.encoder.layer.7.attention.self.query.weight
bert.encoder.layer.7.attention.self.query.bias
bert.encoder.layer.7.attention.self.key.weight
bert.encoder.layer.7.attention.self.key.bias
bert.encoder.layer.7.attention.self.value.weight
bert.encoder.layer.7.attention.self.value.bias
bert.encoder.layer.7.attention.output.dense.weight
bert.encoder.layer.7.attention.output.dense.bias
bert.encoder.layer.7.attention.output.LayerNorm.weight
bert.encoder.layer.7.attention.output.LayerNorm.bias
bert.encoder.layer.7.intermediate.dense.weight
bert.encoder.layer.7.intermediate.dense.bias
bert.encoder.layer.7.output.dense.weight
bert.encoder.layer.7.output.dense.bias
bert.encoder.layer.7.output.LayerNorm.weight
bert.encoder.layer.7.output.LayerNorm.bias
bert.encoder.layer.8.attention.self.query.weight
bert.encoder.layer.8.attention.self.query.bias
bert.encoder.layer.8.attention.self.key.weight
bert.encoder.layer.8.attention.self.key.bias
bert.encoder.layer.8.attention.self.value.weight
bert.encoder.layer.8.attention.self.value.bias
bert.encoder.layer.8.attention.output.dense.weight
bert.encoder.layer.8.attention.output.dense.bias
bert.encoder.layer.8.attention.output.LayerNorm.weight
bert.encoder.layer.8.attention.output.LayerNorm.bias
bert.encoder.layer.8.intermediate.dense.weight
bert.encoder.layer.8.intermediate.dense.bias
bert.encoder.layer.8.output.dense.weight
bert.encoder.layer.8.output.dense.bias
bert.encoder.layer.8.output.LayerNorm.weight
bert.encoder.layer.8.output.LayerNorm.bias
bert.encoder.layer.9.attention.self.query.weight
bert.encoder.layer.9.attention.self.query.bias
bert.encoder.layer.9.attention.self.key.weight
bert.encoder.layer.9.attention.self.key.bias
bert.encoder.layer.9.attention.self.value.weight
bert.encoder.layer.9.attention.self.value.bias
bert.encoder.layer.9.attention.output.dense.weight
bert.encoder.layer.9.attention.output.dense.bias
bert.encoder.layer.9.attention.output.LayerNorm.weight
bert.encoder.layer.9.attention.output.LayerNorm.bias
bert.encoder.layer.9.intermediate.dense.weight
bert.encoder.layer.9.intermediate.dense.bias
bert.encoder.layer.9.output.dense.weight
bert.encoder.layer.9.output.dense.bias
bert.encoder.layer.9.output.LayerNorm.weight
bert.encoder.layer.9.output.LayerNorm.bias
bert.encoder.layer.10.attention.self.query.weight
bert.encoder.layer.10.attention.self.query.bias
bert.encoder.layer.10.attention.self.key.weight
bert.encoder.layer.10.attention.self.key.bias
bert.encoder.layer.10.attention.self.value.weight
bert.encoder.layer.10.attention.self.value.bias
bert.encoder.layer.10.attention.output.dense.weight
bert.encoder.layer.10.attention.output.dense.bias
bert.encoder.layer.10.attention.output.LayerNorm.weight
bert.encoder.layer.10.attention.output.LayerNorm.bias
bert.encoder.layer.10.intermediate.dense.weight
bert.encoder.layer.10.intermediate.dense.bias
bert.encoder.layer.10.output.dense.weight
bert.encoder.layer.10.output.dense.bias
bert.encoder.layer.10.output.LayerNorm.weight
bert.encoder.layer.10.output.LayerNorm.bias
bert.encoder.layer.11.attention.self.query.weight
bert.encoder.layer.11.attention.self.query.bias
bert.encoder.layer.11.attention.self.key.weight
bert.encoder.layer.11.attention.self.key.bias
bert.encoder.layer.11.attention.self.value.weight
bert.encoder.layer.11.attention.self.value.bias
bert.encoder.layer.11.attention.output.dense.weight
bert.encoder.layer.11.attention.output.dense.bias
bert.encoder.layer.11.attention.output.LayerNorm.weight
bert.encoder.layer.11.attention.output.LayerNorm.bias
bert.encoder.layer.11.intermediate.dense.weight
bert.encoder.layer.11.intermediate.dense.bias
bert.encoder.layer.11.output.dense.weight
bert.encoder.layer.11.output.dense.bias
bert.encoder.layer.11.output.LayerNorm.weight
bert.encoder.layer.11.output.LayerNorm.bias
bert.pooler.dense.weight
bert.pooler.dense.bias
lstm.weight_ih_l0
lstm.weight_hh_l0
lstm.bias_ih_l0
lstm.bias_hh_l0
label.weight
label.bias
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 01 | Epoch Time: 1m 51s
	Train Loss: 1.043 | Train Acc: 44.54%
	 Val. Loss: 0.980 |  Val. Acc: 48.30%
tensor([0.5907, 0.7520, 0.0413, 0.3992])
tensor([[670., 447.,  11.],
        [242., 738.,   2.],
        [724., 130.,  36.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 01 | Epoch Time: 1m 48s
	Train Loss: 1.040 | Train Acc: 43.26%
	 Val. Loss: 1.028 |  Val. Acc: 43.12%
tensor([0.1729, 0.9184, 0.2066, 0.3660])
tensor([[202., 840.,  86.],
        [ 54., 900.,  28.],
        [231., 471., 188.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 02 | Epoch Time: 1m 49s
	Train Loss: 0.974 | Train Acc: 50.00%
	 Val. Loss: 0.922 |  Val. Acc: 54.28%
tensor([0.1973, 0.7529, 0.7293, 0.5101])
tensor([[231., 406., 491.],
        [ 98., 738., 146.],
        [143.,  91., 656.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 02 | Epoch Time: 1m 47s
	Train Loss: 0.961 | Train Acc: 51.18%
	 Val. Loss: 0.910 |  Val. Acc: 54.10%
tensor([0.2285, 0.6325, 0.7946, 0.5112])
tensor([[266., 268., 594.],
        [157., 619., 206.],
        [126.,  36., 728.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 03 | Epoch Time: 1m 50s
	Train Loss: 0.953 | Train Acc: 51.97%
	 Val. Loss: 0.902 |  Val. Acc: 56.05%
tensor([0.3393, 0.7368, 0.6333, 0.5481])
tensor([[385., 379., 364.],
        [147., 721., 114.],
        [245.,  78., 567.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 03 | Epoch Time: 1m 47s
	Train Loss: 0.950 | Train Acc: 51.93%
	 Val. Loss: 0.900 |  Val. Acc: 55.15%
tensor([0.3904, 0.7208, 0.5449, 0.5413])
tensor([[443., 388., 297.],
        [190., 706.,  86.],
        [317.,  74., 499.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 04 | Epoch Time: 1m 50s
	Train Loss: 0.943 | Train Acc: 53.15%
	 Val. Loss: 0.905 |  Val. Acc: 55.92%
tensor([0.5351, 0.7378, 0.3735, 0.5423])
tensor([[610., 371., 147.],
        [218., 721.,  43.],
        [473.,  75., 342.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 04 | Epoch Time: 1m 47s
	Train Loss: 0.932 | Train Acc: 53.73%
	 Val. Loss: 0.883 |  Val. Acc: 55.92%
tensor([0.3584, 0.6505, 0.6802, 0.5481])
tensor([[408., 286., 434.],
        [218., 638., 126.],
        [235.,  37., 618.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 05 | Epoch Time: 1m 50s
	Train Loss: 0.927 | Train Acc: 54.08%
	 Val. Loss: 0.895 |  Val. Acc: 56.65%
tensor([0.4173, 0.7739, 0.5006, 0.5519])
tensor([[476., 422., 230.],
        [152., 759.,  71.],
        [335.,  92., 463.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 05 | Epoch Time: 1m 47s
	Train Loss: 0.930 | Train Acc: 53.99%
	 Val. Loss: 0.879 |  Val. Acc: 57.61%
tensor([0.5586, 0.6231, 0.5086, 0.5644])
tensor([[641., 238., 249.],
        [294., 610.,  78.],
        [394.,  30., 466.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 06 | Epoch Time: 1m 50s
	Train Loss: 0.921 | Train Acc: 54.31%
	 Val. Loss: 0.870 |  Val. Acc: 57.92%
tensor([0.3271, 0.6911, 0.7492, 0.5623])
tensor([[382., 286., 460.],
        [166., 675., 141.],
        [167.,  47., 676.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 06 | Epoch Time: 1m 47s
	Train Loss: 0.927 | Train Acc: 53.95%
	 Val. Loss: 0.896 |  Val. Acc: 55.10%
tensor([0.1884, 0.7689, 0.7355, 0.5122])
tensor([[220., 429., 479.],
        [ 78., 754., 150.],
        [113., 106., 671.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 07 | Epoch Time: 1m 51s
	Train Loss: 0.917 | Train Acc: 54.79%
	 Val. Loss: 0.869 |  Val. Acc: 59.05%
tensor([0.4515, 0.7136, 0.6026, 0.5803])
tensor([[520., 324., 284.],
        [190., 698.,  94.],
        [287.,  52., 551.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 07 | Epoch Time: 1m 48s
	Train Loss: 0.919 | Train Acc: 55.10%
	 Val. Loss: 0.872 |  Val. Acc: 57.19%
tensor([0.3525, 0.6483, 0.7241, 0.5533])
tensor([[417., 258., 453.],
        [196., 635., 151.],
        [191.,  39., 660.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 08 | Epoch Time: 1m 50s
	Train Loss: 0.905 | Train Acc: 55.47%
	 Val. Loss: 0.878 |  Val. Acc: 57.53%
tensor([0.3166, 0.6427, 0.8043, 0.5578])
tensor([[368., 228., 532.],
        [181., 630., 171.],
        [132.,  35., 723.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 08 | Epoch Time: 1m 48s
	Train Loss: 0.918 | Train Acc: 54.43%
	 Val. Loss: 0.874 |  Val. Acc: 57.39%
tensor([0.5732, 0.6742, 0.4313, 0.5573])
tensor([[663., 282., 183.],
        [270., 659.,  53.],
        [443.,  51., 396.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 09 | Epoch Time: 1m 50s
	Train Loss: 0.901 | Train Acc: 55.85%
	 Val. Loss: 0.855 |  Val. Acc: 59.98%
tensor([0.4924, 0.6838, 0.6103, 0.5899])
tensor([[566., 274., 288.],
        [216., 670.,  96.],
        [286.,  45., 559.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 09 | Epoch Time: 1m 47s
	Train Loss: 0.907 | Train Acc: 54.92%
	 Val. Loss: 0.874 |  Val. Acc: 57.57%
tensor([0.6039, 0.6283, 0.4515, 0.5623])
tensor([[695., 239., 194.],
        [312., 615.,  55.],
        [443.,  36., 411.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 10 | Epoch Time: 1m 50s
	Train Loss: 0.902 | Train Acc: 56.15%
	 Val. Loss: 0.870 |  Val. Acc: 59.61%
tensor([0.4277, 0.7082, 0.6549, 0.5851])
tensor([[492., 297., 339.],
        [176., 692., 114.],
        [242.,  51., 597.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 10 | Epoch Time: 1m 48s
	Train Loss: 0.909 | Train Acc: 55.61%
	 Val. Loss: 0.863 |  Val. Acc: 58.12%
tensor([0.3863, 0.6845, 0.6738, 0.5662])
tensor([[451., 291., 386.],
        [187., 669., 126.],
        [223.,  49., 618.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 11 | Epoch Time: 1m 51s
	Train Loss: 0.888 | Train Acc: 57.45%
	 Val. Loss: 0.870 |  Val. Acc: 59.86%
tensor([0.4726, 0.7205, 0.5887, 0.5877])
tensor([[545., 325., 258.],
        [190., 704.,  88.],
        [287.,  58., 545.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 11 | Epoch Time: 1m 46s
	Train Loss: 0.904 | Train Acc: 55.98%
	 Val. Loss: 0.861 |  Val. Acc: 58.92%
tensor([0.5348, 0.5909, 0.6181, 0.5802])
tensor([[617., 201., 310.],
        [297., 576., 109.],
        [296.,  27., 567.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 12 | Epoch Time: 1m 51s
	Train Loss: 0.886 | Train Acc: 57.24%
	 Val. Loss: 0.880 |  Val. Acc: 58.26%
tensor([0.3754, 0.7504, 0.6251, 0.5666])
tensor([[435., 376., 317.],
        [144., 735., 103.],
        [232.,  78., 580.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 12 | Epoch Time: 1m 47s
	Train Loss: 0.904 | Train Acc: 55.54%
	 Val. Loss: 0.876 |  Val. Acc: 57.40%
tensor([0.6574, 0.6437, 0.3710, 0.5556])
tensor([[757., 251., 120.],
        [312., 628.,  42.],
        [508.,  50., 332.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 13 | Epoch Time: 1m 51s
	Train Loss: 0.883 | Train Acc: 57.92%
	 Val. Loss: 0.868 |  Val. Acc: 59.59%
tensor([0.4920, 0.7325, 0.5443, 0.5851])
tensor([[563., 340., 225.],
        [187., 717.,  78.],
        [321.,  66., 503.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 13 | Epoch Time: 1m 47s
	Train Loss: 0.903 | Train Acc: 56.02%
	 Val. Loss: 0.858 |  Val. Acc: 59.33%
tensor([0.4686, 0.6576, 0.6463, 0.5824])
tensor([[543., 259., 326.],
        [221., 643., 118.],
        [258.,  43., 589.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 14 | Epoch Time: 1m 50s
	Train Loss: 0.881 | Train Acc: 58.08%
	 Val. Loss: 0.856 |  Val. Acc: 60.66%
tensor([0.4726, 0.6523, 0.6912, 0.5971])
tensor([[546., 235., 347.],
        [229., 639., 114.],
        [219.,  40., 631.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 14 | Epoch Time: 1m 48s
	Train Loss: 0.906 | Train Acc: 55.26%
	 Val. Loss: 0.870 |  Val. Acc: 57.97%
tensor([0.3855, 0.7241, 0.6338, 0.5631])
tensor([[451., 341., 336.],
        [160., 708., 114.],
        [235.,  77., 578.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 15 | Epoch Time: 1m 49s
	Train Loss: 0.871 | Train Acc: 58.49%
	 Val. Loss: 0.854 |  Val. Acc: 61.02%
tensor([0.5594, 0.6789, 0.5672, 0.6022])
tensor([[644., 254., 230.],
        [241., 666.,  75.],
        [326.,  47., 517.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 15 | Epoch Time: 1m 47s
	Train Loss: 0.895 | Train Acc: 56.29%
	 Val. Loss: 0.877 |  Val. Acc: 58.03%
tensor([0.4623, 0.7498, 0.5168, 0.5674])
tensor([[533., 368., 227.],
        [174., 734.,  74.],
        [324.,  94., 472.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 16 | Epoch Time: 1m 50s
	Train Loss: 0.873 | Train Acc: 58.52%
	 Val. Loss: 0.861 |  Val. Acc: 61.30%
tensor([0.4980, 0.6549, 0.6779, 0.6041])
tensor([[575., 231., 322.],
        [225., 642., 115.],
        [231.,  39., 620.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 16 | Epoch Time: 1m 47s
	Train Loss: 0.894 | Train Acc: 56.36%
	 Val. Loss: 0.895 |  Val. Acc: 57.52%
tensor([0.3294, 0.7526, 0.6603, 0.5567])
tensor([[386., 371., 371.],
        [129., 737., 116.],
        [203.,  84., 603.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 17 | Epoch Time: 1m 51s
	Train Loss: 0.871 | Train Acc: 58.68%
	 Val. Loss: 0.861 |  Val. Acc: 60.85%
tensor([0.4527, 0.6706, 0.7025, 0.5979])
tensor([[526., 249., 353.],
        [200., 658., 124.],
        [206.,  45., 639.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 17 | Epoch Time: 1m 47s
	Train Loss: 0.890 | Train Acc: 56.39%
	 Val. Loss: 0.852 |  Val. Acc: 59.68%
tensor([0.5616, 0.6319, 0.5677, 0.5878])
tensor([[648., 222., 258.],
        [277., 616.,  89.],
        [321.,  46., 523.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 18 | Epoch Time: 1m 50s
	Train Loss: 0.867 | Train Acc: 58.50%
	 Val. Loss: 0.864 |  Val. Acc: 60.43%
tensor([0.4648, 0.7148, 0.6207, 0.5931])
tensor([[536., 301., 291.],
        [187., 701.,  94.],
        [252.,  65., 573.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 18 | Epoch Time: 1m 47s
	Train Loss: 0.883 | Train Acc: 57.08%
	 Val. Loss: 0.882 |  Val. Acc: 58.83%
tensor([0.4923, 0.7337, 0.5252, 0.5772])
tensor([[566., 338., 224.],
        [191., 718.,  73.],
        [332.,  81., 477.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 19 | Epoch Time: 1m 50s
	Train Loss: 0.862 | Train Acc: 58.93%
	 Val. Loss: 0.870 |  Val. Acc: 60.22%
tensor([0.4329, 0.6682, 0.7099, 0.5907])
tensor([[504., 254., 370.],
        [200., 655., 127.],
        [198.,  46., 646.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 19 | Epoch Time: 1m 47s
	Train Loss: 0.888 | Train Acc: 56.57%
	 Val. Loss: 0.893 |  Val. Acc: 58.31%
tensor([0.3597, 0.8080, 0.5848, 0.5602])
tensor([[419., 440., 269.],
        [101., 794.,  87.],
        [224., 129., 537.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 20 | Epoch Time: 1m 51s
	Train Loss: 0.868 | Train Acc: 58.43%
	 Val. Loss: 0.872 |  Val. Acc: 60.78%
tensor([0.4468, 0.6707, 0.7098, 0.5976])
tensor([[519., 248., 361.],
        [201., 657., 124.],
        [199.,  46., 645.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 20 | Epoch Time: 1m 47s
	Train Loss: 0.886 | Train Acc: 57.07%
	 Val. Loss: 0.881 |  Val. Acc: 59.32%
tensor([0.3503, 0.7030, 0.7507, 0.5763])
tensor([[408., 291., 429.],
        [152., 687., 143.],
        [156.,  58., 676.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 21 | Epoch Time: 1m 51s
	Train Loss: 0.855 | Train Acc: 59.60%
	 Val. Loss: 0.873 |  Val. Acc: 60.43%
tensor([0.4855, 0.6670, 0.6552, 0.5956])
tensor([[562., 241., 325.],
        [220., 654., 108.],
        [253.,  43., 594.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 21 | Epoch Time: 1m 46s
	Train Loss: 0.879 | Train Acc: 57.32%
	 Val. Loss: 0.855 |  Val. Acc: 59.77%
tensor([0.5811, 0.6402, 0.5460, 0.5898])
tensor([[666., 232., 230.],
        [279., 624.,  79.],
        [354.,  40., 496.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 22 | Epoch Time: 1m 51s
	Train Loss: 0.843 | Train Acc: 60.30%
	 Val. Loss: 0.871 |  Val. Acc: 60.27%
tensor([0.4934, 0.6745, 0.6339, 0.5956])
tensor([[570., 250., 308.],
        [221., 661., 100.],
        [268.,  49., 573.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 22 | Epoch Time: 1m 47s
	Train Loss: 0.884 | Train Acc: 57.32%
	 Val. Loss: 0.866 |  Val. Acc: 59.69%
tensor([0.4850, 0.6527, 0.6486, 0.5888])
tensor([[560., 254., 314.],
        [238., 637., 107.],
        [249.,  52., 589.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 23 | Epoch Time: 1m 52s
	Train Loss: 0.857 | Train Acc: 58.92%
	 Val. Loss: 0.866 |  Val. Acc: 60.64%
tensor([0.5401, 0.6548, 0.6005, 0.5995])
tensor([[621., 237., 270.],
        [246., 642.,  94.],
        [299.,  40., 551.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 23 | Epoch Time: 1m 47s
	Train Loss: 0.880 | Train Acc: 57.56%
	 Val. Loss: 0.863 |  Val. Acc: 60.22%
tensor([0.5303, 0.6621, 0.5907, 0.5935])
tensor([[612., 255., 261.],
        [250., 646.,  86.],
        [290.,  57., 543.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/cnn_model.txt
Epoch: 24 | Epoch Time: 1m 50s
	Train Loss: 0.845 | Train Acc: 60.12%
	 Val. Loss: 0.874 |  Val. Acc: 61.43%
tensor([0.5793, 0.6035, 0.6311, 0.6080])
tensor([[661., 171., 296.],
        [288., 591., 103.],
        [286.,  23., 581.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 24 | Epoch Time: 1m 47s
	Train Loss: 0.874 | Train Acc: 57.72%
	 Val. Loss: 0.860 |  Val. Acc: 60.16%
tensor([0.5147, 0.7136, 0.5591, 0.5913])
tensor([[595., 303., 230.],
        [206., 697.,  79.],
        [309.,  70., 511.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 25 | Epoch Time: 1m 51s
	Train Loss: 0.845 | Train Acc: 60.26%
	 Val. Loss: 0.893 |  Val. Acc: 59.55%
tensor([0.3622, 0.7086, 0.7352, 0.5791])
tensor([[423., 299., 406.],
        [157., 694., 131.],
        [159.,  65., 666.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 25 | Epoch Time: 1m 47s
	Train Loss: 0.877 | Train Acc: 57.94%
	 Val. Loss: 0.867 |  Val. Acc: 60.38%
tensor([0.4716, 0.6724, 0.6646, 0.5956])
tensor([[546., 270., 312.],
        [219., 656., 107.],
        [227.,  59., 604.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 26 | Epoch Time: 1m 51s
	Train Loss: 0.839 | Train Acc: 60.28%
	 Val. Loss: 0.884 |  Val. Acc: 60.99%
tensor([0.5424, 0.5951, 0.6732, 0.6036])
tensor([[624., 170., 334.],
        [286., 582., 114.],
        [251.,  24., 615.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 26 | Epoch Time: 1m 47s
	Train Loss: 0.870 | Train Acc: 58.13%
	 Val. Loss: 0.876 |  Val. Acc: 59.97%
tensor([0.3546, 0.6974, 0.7678, 0.5808])
tensor([[414., 282., 432.],
        [147., 682., 153.],
        [141.,  54., 695.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 27 | Epoch Time: 1m 51s
	Train Loss: 0.845 | Train Acc: 59.98%
	 Val. Loss: 0.902 |  Val. Acc: 60.80%
tensor([0.4764, 0.6111, 0.7347, 0.5992])
tensor([[549., 183., 396.],
        [249., 598., 135.],
        [188.,  34., 668.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 27 | Epoch Time: 1m 47s
	Train Loss: 0.876 | Train Acc: 57.74%
	 Val. Loss: 0.868 |  Val. Acc: 59.32%
tensor([0.4417, 0.6176, 0.7205, 0.5834])
tensor([[512., 225., 391.],
        [244., 605., 133.],
        [187.,  40., 663.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 28 | Epoch Time: 1m 51s
	Train Loss: 0.837 | Train Acc: 61.00%
	 Val. Loss: 0.880 |  Val. Acc: 60.56%
tensor([0.5165, 0.6893, 0.6040, 0.5978])
tensor([[594., 270., 264.],
        [214., 675.,  93.],
        [279.,  66., 545.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 28 | Epoch Time: 1m 47s
	Train Loss: 0.870 | Train Acc: 58.29%
	 Val. Loss: 0.860 |  Val. Acc: 60.36%
tensor([0.4355, 0.7211, 0.6572, 0.5932])
tensor([[505., 313., 310.],
        [168., 706., 108.],
        [240.,  57., 593.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 29 | Epoch Time: 1m 49s
	Train Loss: 0.837 | Train Acc: 60.58%
	 Val. Loss: 0.894 |  Val. Acc: 60.70%
tensor([0.6901, 0.5920, 0.4902, 0.5980])
tensor([[785., 169., 174.],
        [340., 579.,  63.],
        [418.,  24., 448.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 29 | Epoch Time: 1m 47s
	Train Loss: 0.865 | Train Acc: 58.56%
	 Val. Loss: 0.877 |  Val. Acc: 59.94%
tensor([0.4360, 0.6748, 0.6878, 0.5895])
tensor([[501., 270., 357.],
        [203., 662., 117.],
        [210.,  48., 632.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 30 | Epoch Time: 1m 50s
	Train Loss: 0.833 | Train Acc: 60.82%
	 Val. Loss: 0.887 |  Val. Acc: 61.00%
tensor([0.6557, 0.6116, 0.5159, 0.6005])
tensor([[749., 185., 194.],
        [309., 598.,  75.],
        [378.,  34., 478.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 30 | Epoch Time: 1m 47s
	Train Loss: 0.862 | Train Acc: 58.70%
	 Val. Loss: 0.869 |  Val. Acc: 60.09%
tensor([0.4398, 0.6675, 0.7041, 0.5920])
tensor([[509., 250., 369.],
        [214., 651., 117.],
        [204.,  49., 637.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 31 | Epoch Time: 1m 51s
	Train Loss: 0.833 | Train Acc: 60.98%
	 Val. Loss: 0.885 |  Val. Acc: 61.02%
tensor([0.5291, 0.6967, 0.5949, 0.6026])
tensor([[607., 271., 250.],
        [214., 682.,  86.],
        [289.,  63., 538.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 31 | Epoch Time: 1m 47s
	Train Loss: 0.861 | Train Acc: 58.73%
	 Val. Loss: 0.874 |  Val. Acc: 59.76%
tensor([0.4576, 0.6572, 0.6800, 0.5896])
tensor([[528., 249., 351.],
        [231., 640., 111.],
        [221.,  54., 615.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 32 | Epoch Time: 1m 50s
	Train Loss: 0.832 | Train Acc: 61.06%
	 Val. Loss: 0.888 |  Val. Acc: 60.95%
tensor([0.6699, 0.5992, 0.5180, 0.6017])
tensor([[762., 176., 190.],
        [331., 586.,  65.],
        [388.,  33., 469.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 32 | Epoch Time: 1m 47s
	Train Loss: 0.862 | Train Acc: 58.68%
	 Val. Loss: 0.882 |  Val. Acc: 59.95%
tensor([0.4580, 0.6873, 0.6522, 0.5900])
tensor([[529., 290., 309.],
        [209., 670., 103.],
        [219.,  76., 595.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 33 | Epoch Time: 1m 50s
	Train Loss: 0.819 | Train Acc: 61.49%
	 Val. Loss: 0.904 |  Val. Acc: 60.60%
tensor([0.5589, 0.5910, 0.6516, 0.6017])
tensor([[641., 175., 312.],
        [299., 578., 105.],
        [273.,  27., 590.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 33 | Epoch Time: 1m 47s
	Train Loss: 0.855 | Train Acc: 59.53%
	 Val. Loss: 0.885 |  Val. Acc: 60.01%
tensor([0.4259, 0.7274, 0.6481, 0.5875])
tensor([[493., 325., 310.],
        [167., 711., 104.],
        [206.,  92., 592.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 34 | Epoch Time: 1m 50s
	Train Loss: 0.825 | Train Acc: 61.97%
	 Val. Loss: 0.897 |  Val. Acc: 61.04%
tensor([0.6440, 0.6172, 0.5345, 0.6043])
tensor([[737., 206., 185.],
        [307., 605.,  70.],
        [372.,  35., 483.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 34 | Epoch Time: 1m 48s
	Train Loss: 0.855 | Train Acc: 59.63%
	 Val. Loss: 0.917 |  Val. Acc: 59.66%
tensor([0.4507, 0.6751, 0.6686, 0.5885])
tensor([[521., 283., 324.],
        [222., 658., 102.],
        [215.,  70., 605.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 35 | Epoch Time: 1m 50s
	Train Loss: 0.819 | Train Acc: 62.31%
	 Val. Loss: 0.916 |  Val. Acc: 60.78%
tensor([0.4882, 0.6131, 0.7203, 0.6008])
tensor([[565., 198., 365.],
        [258., 599., 125.],
        [202.,  35., 653.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
../checkpoint/attention_model.txt
Epoch: 35 | Epoch Time: 1m 48s
	Train Loss: 0.854 | Train Acc: 59.42%
	 Val. Loss: 0.878 |  Val. Acc: 60.51%
tensor([0.5017, 0.6706, 0.6380, 0.5993])
tensor([[574., 266., 288.],
        [227., 655., 100.],
        [255.,  54., 581.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 36 | Epoch Time: 1m 51s
	Train Loss: 0.817 | Train Acc: 62.30%
	 Val. Loss: 0.917 |  Val. Acc: 60.09%
tensor([0.7121, 0.5556, 0.4849, 0.5923])
tensor([[813., 163., 152.],
        [380., 543.,  59.],
        [427.,  22., 441.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 36 | Epoch Time: 1m 47s
	Train Loss: 0.846 | Train Acc: 59.70%
	 Val. Loss: 0.906 |  Val. Acc: 59.44%
tensor([0.4951, 0.6851, 0.5996, 0.5866])
tensor([[567., 306., 255.],
        [226., 669.,  87.],
        [272.,  78., 540.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 37 | Epoch Time: 1m 51s
	Train Loss: 0.820 | Train Acc: 61.20%
	 Val. Loss: 0.905 |  Val. Acc: 60.58%
tensor([0.5024, 0.6425, 0.6633, 0.5994])
tensor([[579., 235., 314.],
        [242., 629., 111.],
        [238.,  49., 603.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 37 | Epoch Time: 1m 47s
	Train Loss: 0.847 | Train Acc: 59.75%
	 Val. Loss: 0.905 |  Val. Acc: 59.08%
tensor([0.5316, 0.6201, 0.6102, 0.5870])
tensor([[607., 253., 268.],
        [288., 605.,  89.],
        [267.,  69., 554.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 38 | Epoch Time: 1m 50s
	Train Loss: 0.815 | Train Acc: 62.14%
	 Val. Loss: 0.922 |  Val. Acc: 60.65%
tensor([0.6350, 0.6555, 0.4821, 0.5960])
tensor([[727., 233., 168.],
        [278., 642.,  62.],
        [390.,  52., 448.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 38 | Epoch Time: 1m 47s
	Train Loss: 0.854 | Train Acc: 58.79%
	 Val. Loss: 0.865 |  Val. Acc: 60.24%
tensor([0.4771, 0.6914, 0.6339, 0.5953])
tensor([[549., 294., 285.],
        [206., 675., 101.],
        [258.,  53., 579.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 39 | Epoch Time: 1m 51s
	Train Loss: 0.812 | Train Acc: 62.52%
	 Val. Loss: 0.939 |  Val. Acc: 60.35%
tensor([0.4418, 0.6374, 0.7346, 0.5933])
tensor([[514., 224., 390.],
        [231., 624., 127.],
        [176.,  47., 667.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 39 | Epoch Time: 1m 48s
	Train Loss: 0.845 | Train Acc: 60.08%
	 Val. Loss: 0.904 |  Val. Acc: 59.28%
tensor([0.4849, 0.7417, 0.5357, 0.5805])
tensor([[559., 370., 199.],
        [180., 725.,  77.],
        [300.,  98., 492.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 40 | Epoch Time: 1m 51s
	Train Loss: 0.809 | Train Acc: 62.93%
	 Val. Loss: 0.943 |  Val. Acc: 60.43%
tensor([0.4543, 0.6435, 0.7049, 0.5926])
tensor([[528., 232., 368.],
        [235., 630., 117.],
        [193.,  50., 647.]], dtype=torch.float64)
<torchtext.data.iterator.BucketIterator object at 0x7fb127e75b90>
Epoch: 40 | Epoch Time: 1m 47s
	Train Loss: 0.846 | Train Acc: 59.24%
	 Val. Loss: 0.901 |  Val. Acc: 58.77%
tensor([0.6609, 0.5272, 0.5331, 0.5793])
tensor([[756., 179., 193.],
        [396., 514.,  72.],
        [374.,  27., 489.]], dtype=torch.float64)
tensor([[698., 143., 259.],
        [327., 632.,  41.],
        [250.,  19., 630.]], dtype=torch.float64)
	 Val. Loss: 0.948 |  Val. Acc: 65.14%
tensor([0.6368, 0.6294, 0.6623, 0.6422])